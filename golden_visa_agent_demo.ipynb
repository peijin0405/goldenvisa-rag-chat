{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889cf999-52d5-48f0-b2ae-bd9068638222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import sys\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "login(token= hf_token ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7dc521e-7d00-4875-b96b-e953ce6a39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6e9d4d-569a-4c10-8338-4bc904d5c2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\æ•°å­—åŒ–0512\\\\è‘¡è„ç‰™é»„é‡‘ç­¾è¯agent'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219982fe-8490-4272-8b17-1bc1b3918ae0",
   "metadata": {},
   "source": [
    "#### Split the doucment into Chunks & Store them in Vector Store\n",
    "æŠŠ PDF æ–‡ä»¶å¤„ç†æˆã€Œå¯æœç´¢å¯è°ƒç”¨ã€çš„å‘é‡æ•°æ®åº“ï¼Œä¾›åç»­é—®ç­”ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8ef06b-8961-469b-8049-8f1890e17ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x254b796f9a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./data/___Lei n.Âº 23_2007, de 04 de Julho.pdf\")\n",
    "loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c42b47-a38e-4e85-882e-9ae79fb5d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest():\n",
    "    # Get the doc\n",
    "    loader = PyPDFLoader(\"./data/___Lei n.Âº 23_2007, de 04 de Julho.pdf\")\n",
    "    pages = loader.load_and_split() ##åŠ è½½PDFæ–‡æ¡£ï¼Œå¹¶æŒ‰ç…§é¡µè‡ªåŠ¨åˆ†ä¸ºpagesï¼Œæ¯é¡µæ˜¯ä¸€ä¸ªdocumentå¯¹è±¡\n",
    "    # Split the pages by char\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,## æŠŠæ¯é¡µåˆ‡æˆ1024å­—ç¬¦çš„æ®µè½ï¼Œå¸¦100å­—é‡å ï¼Œç›®çš„æ˜¯é¿å…è¯­ä¹‰æ–­è£‚\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True, ## ä¸ºäº†è®°å½•åŸå§‹æ–‡æœ¬ä½ç½®ï¼Œå¯æº¯æº\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Split {len(pages)} documents into {len(chunks)} chunks.\")\n",
    "    # åµŒå…¥æ¨¡å‹åˆå§‹åŒ–ï¼Œä½¿ç”¨bge-m3æ¨¡å‹åµŒå…¥ï¼Œå°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚bge-m3æ¨¡å‹æ”¯æŒå¤šè¯­æŸ¥è¯¢\n",
    "    #embedding = FastEmbedEmbeddings()\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    #Create vector store ç”¨å‘é‡åŒ–ä¹‹åçš„chunksæ„å»ºä¸€ä¸ªchromaæ•°æ®åº“ï¼Œå¹¶æŒä¹…åŒ–åœ¨./sql_chroma_dbè·¯å¾„ä¸‹é¢ï¼Œåç»­è°ƒç”¨æ— éœ€é‡æ–°å¤„ç†\n",
    "    Chroma.from_documents(documents=chunks,  embedding=embedding, persist_directory=\"./goldenvisa_chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137f4d93-cd63-4383-a622-0597a68551ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 166 documents into 714 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peijin\\AppData\\Local\\Temp\\ipykernel_32948\\211745222.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# only run this once to generate vector store\n",
    "ingest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d9dca-bb5c-4385-a46b-7b7fa4789f6b",
   "metadata": {},
   "source": [
    "##### Create a RAG chain that retreives relevent chunks and prepares a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b9c506-f00a-4c7a-8dfa-45bf3df299b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peijin\\AppData\\Local\\Temp\\ipykernel_32948\\204113417.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# æŒ‡å®šæ¨¡å‹åç§°ï¼Œå¿…é¡»å’Œä½ æ‹‰å–çš„ä¸€è‡´\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# æµ‹è¯•è°ƒç”¨\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f448e1-01e4-4c06-a382-9546f56db071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain==0.1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4abac4d-0e61-45a0-8cd6-0d6b56fab5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import FastEmbedEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.callbacks import StdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16cafe9-8014-42f9-a172-450f4cbdf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5533413c-3b36-4f76-b627-fca639ebe8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# æ„é€  Prompt æ¨¡æ¿\n",
    "def build_prompt():\n",
    "    return PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        <s>[Instructions] You are a helpful assistant. Use only the context below to answer the question.\n",
    "        If the answer is not in the context, say: \"No context available for this question.\"\n",
    "        [/Instructions]</s>\n",
    "\n",
    "        <s>[Input] Question: {input}\n",
    "        Context: {context}\n",
    "        Answer: [/Input]</s>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# åŠ è½½å‘é‡æ•°æ®åº“\n",
    "def load_vector_store():\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    return Chroma(\n",
    "        persist_directory=\"./goldenvisa_chroma_db\",\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "\n",
    "\n",
    "# æ„å»º Retrieverï¼ˆè®¾ç½® k å’Œé˜ˆå€¼ï¼‰\n",
    "def build_retriever(vector_store):\n",
    "    return vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": 3,\n",
    "            \"score_threshold\": 0.5,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "# æå– source ä¿¡æ¯\n",
    "def format_answer_with_sources(answer: str, docs: list[Document]) -> str:\n",
    "    sources = \"\\n\".join([\n",
    "        f\"- Page: {doc.metadata.get('source', 'N/A')}\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "    return f\"{answer.strip()}\\n\\nğŸ“š References:\\n{sources}\"\n",
    "\n",
    "\n",
    "# æ„å»ºå¸¦è¾“å‡º & source çš„å¢å¼ºå‹ RAG chain\n",
    "def rag_chain():\n",
    "    # 1. Load LLM\n",
    "    model = ChatOllama(model=\"llama3\")\n",
    "\n",
    "    # 2. Prompt\n",
    "    prompt = build_prompt()\n",
    "\n",
    "    # 3. Vector store\n",
    "    vector_store = load_vector_store()\n",
    "\n",
    "    # 4. Retriever\n",
    "    retriever = build_retriever(vector_store)\n",
    "\n",
    "    # 5. Stuff chain\n",
    "    document_chain = create_stuff_documents_chain(model, prompt)\n",
    "\n",
    "    # 6. Retrieval chain\n",
    "    chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "    # 7. åŒ…è£…ä¸ºå¸¦æ¥æºè¾“å‡ºçš„æ‰§è¡Œå‡½æ•°\n",
    "    def run_with_sources(user_input: str):\n",
    "        result = chain.invoke(\n",
    "            {\"input\": user_input},\n",
    "            config={\"callbacks\": [StdOutCallbackHandler()]}\n",
    "        )\n",
    "        # è¾“å‡ºä¸­åŒ…æ‹¬ retrieved æ–‡æ¡£ï¼ˆç”¨äºå¼•ç”¨ï¼‰\n",
    "        answer = result[\"answer\"]\n",
    "        docs = result[\"context\"]\n",
    "        return format_answer_with_sources(answer, docs)\n",
    "\n",
    "    return run_with_sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e84637-cffe-4bce-8177-ce0ce766f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableAssign<context> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableParallel<context> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableLambda chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peijin\\miniconda3\\envs\\myenv\\lib\\site-packages\\langchain_core\\vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.5\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableAssign<answer> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableParallel<answer> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableAssign<context> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableParallel<context> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableLambda chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new PromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StrOutputParser chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question: Quais sÃ£o os requisitos para obter o visto gold em Portugal atravÃ©s do reagrupamento familiar?\n",
      "\n",
      "Context:\n",
      "Answer: Os requisitos para obter o visto gold em Portugal atravÃ©s do reagrupamento familiar sÃ£o:\n",
      "\n",
      "* Ser um membro da famÃ­lia de um cidadÃ£o portuguÃªs ou de uma pessoa com direito a residÃªncia permanente em Portugal;\n",
      "* Ter morado continuamente em Portugal pelo menos trÃªs anos consecutivos, exceto se estiver justificado por motivos de saÃºde, educaÃ§Ã£o ou outras circunstÃ¢ncias atenuantes;\n",
      "* Ter um arranjo familiar que envolva a reuniÃ£o da famÃ­lia no paÃ­s;\n",
      "* Possuir recursos suficientes para sustentar a si e os familiares que vÃªm junto;\n",
      "* NÃ£o ser considerado uma ameaÃ§a Ã  seguranÃ§a pÃºblica ou Ã  ordem social portuguesa.\n",
      "\n",
      "Fonte: Governo PortuguÃªs - ServiÃ§o de Estrangeiros e Fronteiras (SEF).\n",
      "\n",
      "ğŸ“š References:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_chain = rag_chain()\n",
    "response = qa_chain(\"Quais sÃ£o os requisitos para obter o visto gold em Portugal atravÃ©s do reagrupamento familiar?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03ebb9-f7a1-47ea-9c69-4459707f101c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c860e-dc2b-4c67-85db-ba393efca17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60eb94-ad86-4eb6-86a5-8a3d00319265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c16667-0dd5-4996-8692-d9451f5b196e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d00d2e-0b68-449b-a3b4-9975d98218fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7859a6-42ab-4624-af2d-ad19585340b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec95c1-ca7e-44d7-bfc7-1a5ee052c41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
