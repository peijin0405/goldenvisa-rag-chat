{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889cf999-52d5-48f0-b2ae-bd9068638222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import sys\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "login(token= hf_token ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7dc521e-7d00-4875-b96b-e953ce6a39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6e9d4d-569a-4c10-8338-4bc904d5c2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\数字化0512\\\\葡萄牙黄金签证agent'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219982fe-8490-4272-8b17-1bc1b3918ae0",
   "metadata": {},
   "source": [
    "#### Split the doucment into Chunks & Store them in Vector Store\n",
    "把 PDF 文件处理成「可搜索可调用」的向量数据库，供后续问答使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8ef06b-8961-469b-8049-8f1890e17ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x254b796f9a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./data/___Lei n.º 23_2007, de 04 de Julho.pdf\")\n",
    "loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c42b47-a38e-4e85-882e-9ae79fb5d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest():\n",
    "    # Get the doc\n",
    "    loader = PyPDFLoader(\"./data/___Lei n.º 23_2007, de 04 de Julho.pdf\")\n",
    "    pages = loader.load_and_split() ##加载PDF文档，并按照页自动分为pages，每页是一个document对象\n",
    "    # Split the pages by char\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,## 把每页切成1024字符的段落，带100字重叠，目的是避免语义断裂\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True, ## 为了记录原始文本位置，可溯源\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Split {len(pages)} documents into {len(chunks)} chunks.\")\n",
    "    # 嵌入模型初始化，使用bge-m3模型嵌入，将文本块转换为向量表示。bge-m3模型支持多语查询\n",
    "    #embedding = FastEmbedEmbeddings()\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    #Create vector store 用向量化之后的chunks构建一个chroma数据库，并持久化在./sql_chroma_db路径下面，后续调用无需重新处理\n",
    "    Chroma.from_documents(documents=chunks,  embedding=embedding, persist_directory=\"./goldenvisa_chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137f4d93-cd63-4383-a622-0597a68551ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 166 documents into 714 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peijin\\AppData\\Local\\Temp\\ipykernel_32948\\211745222.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# only run this once to generate vector store\n",
    "ingest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d9dca-bb5c-4385-a46b-7b7fa4789f6b",
   "metadata": {},
   "source": [
    "##### Create a RAG chain that retreives relevent chunks and prepares a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b9c506-f00a-4c7a-8dfa-45bf3df299b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peijin\\AppData\\Local\\Temp\\ipykernel_32948\\204113417.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# 指定模型名称，必须和你拉取的一致\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# 测试调用\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f448e1-01e4-4c06-a382-9546f56db071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain==0.1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4abac4d-0e61-45a0-8cd6-0d6b56fab5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import FastEmbedEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.callbacks import StdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16cafe9-8014-42f9-a172-450f4cbdf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5533413c-3b36-4f76-b627-fca639ebe8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 构造 Prompt 模板\n",
    "def build_prompt():\n",
    "    return PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        <s>[Instructions] You are a helpful assistant. Use only the context below to answer the question.\n",
    "        If the answer is not in the context, say: \"No context available for this question.\"\n",
    "        [/Instructions]</s>\n",
    "\n",
    "        <s>[Input] Question: {input}\n",
    "        Context: {context}\n",
    "        Answer: [/Input]</s>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 加载向量数据库\n",
    "def load_vector_store():\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    return Chroma(\n",
    "        persist_directory=\"./goldenvisa_chroma_db\",\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "\n",
    "\n",
    "# 构建 Retriever（设置 k 和阈值）\n",
    "def build_retriever(vector_store):\n",
    "    return vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": 3,\n",
    "            \"score_threshold\": 0.5,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "# 提取 source 信息\n",
    "def format_answer_with_sources(answer: str, docs: list[Document]) -> str:\n",
    "    sources = \"\\n\".join([\n",
    "        f\"- Page: {doc.metadata.get('source', 'N/A')}\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "    return f\"{answer.strip()}\\n\\n📚 References:\\n{sources}\"\n",
    "\n",
    "\n",
    "# 构建带输出 & source 的增强型 RAG chain\n",
    "def rag_chain():\n",
    "    # 1. Load LLM\n",
    "    model = ChatOllama(model=\"llama3\")\n",
    "\n",
    "    # 2. Prompt\n",
    "    prompt = build_prompt()\n",
    "\n",
    "    # 3. Vector store\n",
    "    vector_store = load_vector_store()\n",
    "\n",
    "    # 4. Retriever\n",
    "    retriever = build_retriever(vector_store)\n",
    "\n",
    "    # 5. Stuff chain\n",
    "    document_chain = create_stuff_documents_chain(model, prompt)\n",
    "\n",
    "    # 6. Retrieval chain\n",
    "    chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "    # 7. 包装为带来源输出的执行函数\n",
    "    def run_with_sources(user_input: str):\n",
    "        result = chain.invoke(\n",
    "            {\"input\": user_input},\n",
    "            config={\"callbacks\": [StdOutCallbackHandler()]}\n",
    "        )\n",
    "        # 输出中包括 retrieved 文档（用于引用）\n",
    "        answer = result[\"answer\"]\n",
    "        docs = result[\"context\"]\n",
    "        return format_answer_with_sources(answer, docs)\n",
    "\n",
    "    return run_with_sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e84637-cffe-4bce-8177-ce0ce766f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableAssign<context> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableParallel<context> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableLambda chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peijin\\miniconda3\\envs\\myenv\\lib\\site-packages\\langchain_core\\vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.5\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableAssign<answer> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableParallel<answer> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableAssign<context> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableParallel<context> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableLambda chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new PromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StrOutputParser chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question: Quais são os requisitos para obter o visto gold em Portugal através do reagrupamento familiar?\n",
      "\n",
      "Context:\n",
      "Answer: Os requisitos para obter o visto gold em Portugal através do reagrupamento familiar são:\n",
      "\n",
      "* Ser um membro da família de um cidadão português ou de uma pessoa com direito a residência permanente em Portugal;\n",
      "* Ter morado continuamente em Portugal pelo menos três anos consecutivos, exceto se estiver justificado por motivos de saúde, educação ou outras circunstâncias atenuantes;\n",
      "* Ter um arranjo familiar que envolva a reunião da família no país;\n",
      "* Possuir recursos suficientes para sustentar a si e os familiares que vêm junto;\n",
      "* Não ser considerado uma ameaça à segurança pública ou à ordem social portuguesa.\n",
      "\n",
      "Fonte: Governo Português - Serviço de Estrangeiros e Fronteiras (SEF).\n",
      "\n",
      "📚 References:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_chain = rag_chain()\n",
    "response = qa_chain(\"Quais são os requisitos para obter o visto gold em Portugal através do reagrupamento familiar?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03ebb9-f7a1-47ea-9c69-4459707f101c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c860e-dc2b-4c67-85db-ba393efca17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60eb94-ad86-4eb6-86a5-8a3d00319265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c16667-0dd5-4996-8692-d9451f5b196e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d00d2e-0b68-449b-a3b4-9975d98218fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7859a6-42ab-4624-af2d-ad19585340b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec95c1-ca7e-44d7-bfc7-1a5ee052c41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
